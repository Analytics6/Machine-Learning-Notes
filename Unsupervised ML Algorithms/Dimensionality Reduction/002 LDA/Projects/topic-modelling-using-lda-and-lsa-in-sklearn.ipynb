{"cells":[{"metadata":{"_uuid":"9341bf79ac503efb2b0c19cae84b17b48c30e211","id":"x39boSrzycsa"},"cell_type":"markdown","source":["## Topic Modelling using Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA) in sklearn"]},{"metadata":{"trusted":false,"_uuid":"e5d7c8f848246a160104b0002220baf4c95e9eb2","id":"62VRGsODycse","executionInfo":{"status":"ok","timestamp":1701508204216,"user_tz":-330,"elapsed":6,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":[],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f9de996a68ce9d21cb302f795a0c8f667784879","id":"dQcviUQOycsf"},"cell_type":"markdown","source":["### **There also exists implementation using the Gensim library. Checkout the same [here](https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/)  ,   [here](https://nlpforhackers.io/topic-modeling/) and [here](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/) and also in [this](https://github.com/susanli2016/NLP-with-Python/blob/master/LDA_news_headlines.ipynb) notebook.**"]},{"metadata":{"trusted":false,"_uuid":"290af1f20803420483653e4e447732f4658900ec","id":"t6JNyqkNycsg","executionInfo":{"status":"ok","timestamp":1701508204216,"user_tz":-330,"elapsed":5,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":[],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"827e42f1d6fbd87d0858fd90555a280782a2ed73","id":"RXFt6DERycsg"},"cell_type":"markdown","source":["## [Please star/upvote in case u like it. ]"]},{"metadata":{"trusted":false,"_uuid":"87d8d19cf6c49e0af3e33f6ccd77bad5ec5f46a6","id":"o2M1XwQIycsh","executionInfo":{"status":"ok","timestamp":1701508204216,"user_tz":-330,"elapsed":4,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":[],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a503c2851673a3d3ec1eb2c3d6c6e5566292af1","id":"4AGANO8Jycsi"},"cell_type":"markdown","source":["#### IMPORTING MODULES"]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"5O5_-2kTGD6V","trusted":true,"_uuid":"39f140e85d319161012fb67f258dd6b339545b0a","executionInfo":{"status":"error","timestamp":1701508207359,"user_tz":-330,"elapsed":3147,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}},"outputId":"6820ceae-8600-4163-d89f-6866e184179b"},"cell_type":"code","source":["# data visualisation and manipulation\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from matplotlib import style\n","import seaborn as sns\n","#configure\n","# sets matplotlib to inline and displays graphs below the corressponding cell.\n","%matplotlib inline\n","style.use('fivethirtyeight')\n","sns.set(style='whitegrid',color_codes=True)\n","\n","#import nltk\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize,sent_tokenize\n","\n","#preprocessing\n","from nltk.corpus import stopwords  #stopwords\n","from nltk import word_tokenize,sent_tokenize # tokenizing\n","from nltk.stem import PorterStemmer,LancasterStemmer  # using the Porter Stemmer and Lancaster Stemmer and others\n","from nltk.stem.snowball import SnowballStemmer\n","from nltk.stem import WordNetLemmatizer  # lammatizer from WordNet\n","\n","# for named entity recognition (NER)\n","from nltk import ne_chunk\n","\n","# vectorizers for creating the document-term-matrix (DTM)\n","from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n","\n","#stop-words\n","stop_words=set(nltk.corpus.stopwords.words('english'))"],"execution_count":1,"outputs":[{"output_type":"error","ename":"LookupError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-0c0e4f209028>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#stop-words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"]}]},{"metadata":{"trusted":true,"_uuid":"fd492c7bc386ce83719ea3172baf526b9acaaecb","id":"1-30m5-Iycsl","executionInfo":{"status":"aborted","timestamp":1701508207360,"user_tz":-330,"elapsed":28,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":[],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2b520342f1761de78285e76aa6327d9d37b1f4e","id":"NOCbuRsvycsl"},"cell_type":"markdown","source":["#### LOADING THE DATASET"]},{"metadata":{"id":"xaPWCpthHVIS","trusted":true,"_uuid":"aad495ed355e99947bc3b9816cfad0ff430c17e5","executionInfo":{"status":"aborted","timestamp":1701508207361,"user_tz":-330,"elapsed":29,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":["df=pd.read_csv(r'../input/abcnews-date-text.csv')"],"execution_count":null,"outputs":[]},{"metadata":{"id":"Wwf4uKjTIYm2","trusted":true,"_uuid":"e09dd69ffa2bae1920f3e10b1af0a8ce0f8ae16d","executionInfo":{"status":"aborted","timestamp":1701508207362,"user_tz":-330,"elapsed":30,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":["df.head()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"dcaba93fba40631607159165262af6cb846fa501","id":"Kzbu9aXxycsm","executionInfo":{"status":"aborted","timestamp":1701508207363,"user_tz":-330,"elapsed":31,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":[],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ece312ea148c498c5584e83f6a36fce2dc236198","id":"k8xSxnvLycsn"},"cell_type":"markdown","source":["We will drop the **'publish_date'** column as it is useless for our discussion."]},{"metadata":{"id":"wiuusKeEIr_o","trusted":false,"_uuid":"2a6df530624e3b9c3cfb43659ba9c0f67e2aa1d4","executionInfo":{"status":"aborted","timestamp":1701508207363,"user_tz":-330,"elapsed":30,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":["# drop the publish date.\n","df.drop(['publish_date'],axis=1,inplace=True)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"FxegmHWtI3eJ","trusted":false,"_uuid":"bd907b93c352fce35e8e6a886c01e9cf344ce455","executionInfo":{"status":"aborted","timestamp":1701508207364,"user_tz":-330,"elapsed":31,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":["df.head(10)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"81bc140459be85e40f5400b6e26c126385a3d4c6","id":"6yFalMmhycsn","executionInfo":{"status":"aborted","timestamp":1701508207364,"user_tz":-330,"elapsed":31,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":[],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2043c3fc7cb55f29282da53bf8ce8315aec19556","id":"1b5t1gEPycsn"},"cell_type":"markdown","source":["#### DATA CLEANING & PRE-PROCESSING"]},{"metadata":{"_uuid":"dbd9e74a45cef82d83251327066f919c2a7ee08b","id":"IdqnOitCycso"},"cell_type":"markdown","source":["Here I have done the data pre-processing. I have used the lemmatizer and can also use the stemmer. Also the stop words have been used along with the words wit lenght shorter than 3 characters to reduce some stray words."]},{"metadata":{"id":"10MF-YmnI7vp","trusted":false,"_uuid":"c01abf16fc6c3b72873a40246d88f4f2d84af6a6","executionInfo":{"status":"aborted","timestamp":1701508207365,"user_tz":-330,"elapsed":31,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":["def clean_text(headline):\n","  le=WordNetLemmatizer()\n","  word_tokens=word_tokenize(headline)\n","  tokens=[le.lemmatize(w) for w in word_tokens if w not in stop_words and len(w)>3]\n","  cleaned_text=\" \".join(tokens)\n","  return cleaned_text\n","\n",""],"execution_count":null,"outputs":[]},{"metadata":{"id":"J9cX-1YxSisp","trusted":false,"_uuid":"0249f1e9dc0d448aa55778d88b17380cbb099b27","executionInfo":{"status":"aborted","timestamp":1701508207365,"user_tz":-330,"elapsed":31,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":["# time taking\n","df['headline_cleaned_text']=df['headline_text'].apply(clean_text)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"Qw6e2o8SSzi2","trusted":false,"_uuid":"c81efcbf76b11b4a08c18e4ce6147fa35bfd2348","executionInfo":{"status":"aborted","timestamp":1701508207366,"user_tz":-330,"elapsed":32,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":["df.head()"],"execution_count":null,"outputs":[]},{"metadata":{"id":"Wuos0NVsVJFg","_uuid":"79d7fa2c195ec7a4b5410885cea4dbc7e58db316","executionInfo":{"status":"aborted","timestamp":1701508207366,"user_tz":-330,"elapsed":32,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":["Can see the difference after removal of stopwords and some shorter words. aslo the words have been lemmatized as in **'calls'--->'call'.**"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"40712caac928c4a299e7fce7d759f3e9e5555c61","id":"DQ-dx71Kycso","executionInfo":{"status":"aborted","timestamp":1701508207366,"user_tz":-330,"elapsed":31,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":[],"execution_count":null,"outputs":[]},{"metadata":{"id":"RDqxfd7VVhJx","_uuid":"c46d3085571e5bd3224bfb6a51164816a5e2a613","executionInfo":{"status":"aborted","timestamp":1701508207367,"user_tz":-330,"elapsed":32,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":["Now drop the unpre-processed column."],"execution_count":null,"outputs":[]},{"metadata":{"id":"PTkgClkxV7MP","trusted":false,"_uuid":"64cc5de96668ec6e6f37cefb31c349f35ff81771","executionInfo":{"status":"aborted","timestamp":1701508207367,"user_tz":-330,"elapsed":32,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":["df.drop(['headline_text'],axis=1,inplace=True)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"PHPX62iRV_ux","trusted":false,"_uuid":"dc01093ea4f6bd35e492e5136bec2ef90f8a1bf5","executionInfo":{"status":"aborted","timestamp":1701508207368,"user_tz":-330,"elapsed":32,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":["df.head()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9338f9639490e9ef06a76ed818db757d83d6c582","id":"VKeHSFngycsp","executionInfo":{"status":"aborted","timestamp":1701508207368,"user_tz":-330,"elapsed":32,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":[],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac1c4d306d9c1ea25c1b28250b386a8d8fde4134","id":"OiBPqL9Gycsp"},"cell_type":"markdown","source":["We can also see any particular news headline."]},{"metadata":{"id":"zlVvCr7qWkqA","trusted":false,"_uuid":"219d5fa37fbffff1c1acdeb46722a40cee1842a2","executionInfo":{"status":"aborted","timestamp":1701508207368,"user_tz":-330,"elapsed":31,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":["df['headline_cleaned_text'][0]"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"feacf23db532c748405fc7b908de218f2511b931","id":"4Dc_dJf5ycsp","executionInfo":{"status":"aborted","timestamp":1701508207369,"user_tz":-330,"elapsed":32,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":[],"execution_count":null,"outputs":[]},{"metadata":{"id":"yQZpp5YMXPkb","_uuid":"d36edc03b2cc88ef61651cd6c9deb470817e64df","executionInfo":{"status":"aborted","timestamp":1701508207369,"user_tz":-330,"elapsed":32,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":["#### EXTRACTING THE FEATURES AND CREATING THE DOCUMENT-TERM-MATRIX ( DTM )\n","\n","In DTM the values are the TFidf values.\n","\n","Also I have specified some parameters of the Tfidf vectorizer.\n","\n","Some important points:-\n","\n","**1) LSA is generally implemented with Tfidf values everywhere and not with the Count Vectorizer.**\n","\n","**2) max_features depends on your computing power and also on eval. metric (coherence score is a metric for topic model). Try the value that gives best eval. metric and doesn't limits processing power.**\n","\n","**3) Default values for min_df & max_df worked well.**\n","\n","**4) Can try different values for ngram_range.**"],"execution_count":null,"outputs":[]},{"metadata":{"id":"UUxESGqhXX07","trusted":false,"_uuid":"d3998f5b7d7eaed5d18d08830d2ba9ba8dd7fcd6","executionInfo":{"status":"aborted","timestamp":1701508207369,"user_tz":-330,"elapsed":32,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":["vect =TfidfVectorizer(stop_words=stop_words,max_features=1000) # to play with. min_df,max_df,max_features etc..."],"execution_count":null,"outputs":[]},{"metadata":{"id":"z0CezNO4aUoh","trusted":false,"_uuid":"00cdefa5281e43ed4cd28af256490cf3cf4dbaf2","executionInfo":{"status":"aborted","timestamp":1701508207369,"user_tz":-330,"elapsed":31,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":["vect_text=vect.fit_transform(df['headline_cleaned_text'])"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1fda68dd69c429fff879d660aa0ec7dccec18a64","id":"LrlL_6Alycsq","executionInfo":{"status":"aborted","timestamp":1701508207369,"user_tz":-330,"elapsed":31,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":[],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb0975cb5ba0e82f9e14eba53bd466c26b892a7e","id":"Cj2r3HgGycsq"},"cell_type":"markdown","source":["#### We can now see the most frequent and rare words in the news headlines based on idf score. The lesser the value; more common is the word in the news headlines."]},{"metadata":{"id":"5G-Zt5lUsUjx","trusted":false,"_uuid":"a75bc1a06e66fa4c8615822f703487424761570b","executionInfo":{"status":"aborted","timestamp":1701508207370,"user_tz":-330,"elapsed":32,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":["print(vect_text.shape)\n","print(vect_text)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"zbF-rC1va7tP","trusted":false,"_uuid":"fdf6846bf90d386f8361467a420af9b40b689521","executionInfo":{"status":"aborted","timestamp":1701508207370,"user_tz":-330,"elapsed":32,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":["idf=vect.idf_"],"execution_count":null,"outputs":[]},{"metadata":{"id":"a0wQG_tZm53A","trusted":false,"_uuid":"b5dc9f0d99d949a36cdb00be41b5ba4a424f093c","executionInfo":{"status":"aborted","timestamp":1701508207370,"user_tz":-330,"elapsed":32,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":["dd=dict(zip(vect.get_feature_names(), idf))\n","l=sorted(dd, key=(dd).get)\n","# print(l)\n","print(l[0],l[-1])\n","print(dd['police'])\n","print(dd['forecast'])  # police is most common and forecast is least common among the news headlines."],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"868e6b05bc280289edea4e14b31c98a435d8ebbc","id":"zJIFy_F6ycsv"},"cell_type":"markdown","source":["We can therefore see that on the basis of the **idf value** , **'police'** is the **most frequent** word while **'forecast'** is **least frequently** occuring among the news."]},{"metadata":{"trusted":false,"_uuid":"c2ed01a1ea2413b95aa748487111c0ca6cc4e7ec","id":"6iBwi0gDycsv","executionInfo":{"status":"aborted","timestamp":1701508207371,"user_tz":-330,"elapsed":33,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":[],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b96bb00ba3313b3fcab6735e73c56e89fc74365","id":"oUbFS3ccycsv"},"cell_type":"markdown","source":["### TOPIC MODELLING"]},{"metadata":{"trusted":false,"_uuid":"4ce3e769f54ef00290465cd8b1d7bd4089df71ea","id":"5l_nQ5cEycsv","executionInfo":{"status":"aborted","timestamp":1701508207371,"user_tz":-330,"elapsed":33,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":[],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0bad03b7f6b2b69cd4554ec9e7681b430dfe4e10","id":"HGA0n_uZycsv"},"cell_type":"markdown","source":["## Latent Semantic Analysis (LSA)"]},{"metadata":{"_uuid":"c0d4b13524635aefa5e44f780efeec9b55ca6e58","id":"ndmSnvXMycsv"},"cell_type":"markdown","source":["The first approach that I have used is the LSA. **LSA is basically singular value decomposition.**\n","\n","**SVD decomposes the original DTM into three matrices S=U.(sigma).(V.T). Here the matrix U denotes the document-topic matrix while (V) is the topic-term matrix.**\n","\n","**Each row of the matrix U(document-term matrix) is the vector representation of the corresponding document. The length of these vectors is the number of desired topics. Vector representation for the terms in our data can be found in the matrix V (term-topic matrix).**\n","\n","So, SVD gives us vectors for every document and term in our data. The length of each vector would be k. **We can then use these vectors to find similar words and similar documents using the cosine similarity method.**\n","\n","We can use the truncatedSVD function to implement LSA. The n_components parameter is the number of topics we wish to extract.\n","The model is then fit and transformed on the result given by vectorizer.\n","\n","**Lastly note that LSA and LSI (I for indexing) are the same and the later is just sometimes used in information retrieval contexts.**"]},{"metadata":{"id":"9xuGfFdtqTp5","trusted":false,"_uuid":"ac483df5e69eeb2b4b71af956c6987896eda7eb6","executionInfo":{"status":"aborted","timestamp":1701508207371,"user_tz":-330,"elapsed":33,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":["from sklearn.decomposition import TruncatedSVD\n","lsa_model = TruncatedSVD(n_components=10, algorithm='randomized', n_iter=10, random_state=42)\n","\n","lsa_top=lsa_model.fit_transform(vect_text)\n"],"execution_count":null,"outputs":[]},{"metadata":{"id":"KdaVWPC6xmq-","trusted":false,"_uuid":"0a8f4620d3c71db1275e4b5662bf4ab32c598266","executionInfo":{"status":"aborted","timestamp":1701508207371,"user_tz":-330,"elapsed":33,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":["print(lsa_top)\n","print(lsa_top.shape)  # (no_of_doc*no_of_topics)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"d4dvIGIGxtAI","trusted":false,"_uuid":"2a259e5f3b8c11118368439eea8de0faa8b7a1c2","executionInfo":{"status":"aborted","timestamp":1701508207371,"user_tz":-330,"elapsed":32,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":["l=lsa_top[0]\n","print(\"Document 0 :\")\n","for i,topic in enumerate(l):\n","  print(\"Topic \",i,\" : \",topic*100)\n","\n"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2153bcbf45e7623efc213bb94f0dbba377ff34c3","id":"S3IbJC8Rycsw"},"cell_type":"markdown","source":["Similalry for other documents we can do this. However note that values dont add to 1 as in LSA it is not probabiltiy of a topic in a document."]},{"metadata":{"id":"AI2kOuwitOGp","trusted":false,"_uuid":"61bb8ff8b4b6cf0cd2fcde7890f3a3ebb8980452","executionInfo":{"status":"aborted","timestamp":1701508207371,"user_tz":-330,"elapsed":32,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":["print(lsa_model.components_.shape) # (no_of_topics*no_of_words)\n","print(lsa_model.components_)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6f834000fa7b2f294694572fda0bcf56d25badd6","id":"YFlgL6Cvycsw","executionInfo":{"status":"aborted","timestamp":1701508207371,"user_tz":-330,"elapsed":32,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":[],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a289f6e017d0c573d20ed386582df71437fab5cc","id":"DwmPeJ_aycsw"},"cell_type":"markdown","source":["#### Now e can get a list of the important words for each of the 10 topics as shown. For simplicity here I have shown 10 words for each topic."]},{"metadata":{"id":"GMMmSTbQqfdz","trusted":false,"_uuid":"c9860197f6d11e5583629cca322e19a93d4c0350","executionInfo":{"status":"aborted","timestamp":1701508207371,"user_tz":-330,"elapsed":32,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":["# most important words for each topic\n","vocab = vect.get_feature_names()\n","\n","for i, comp in enumerate(lsa_model.components_):\n","    vocab_comp = zip(vocab, comp)\n","    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]\n","    print(\"Topic \"+str(i)+\": \")\n","    for t in sorted_words:\n","        print(t[0],end=\" \")\n","    print(\"\\n\")\n",""],"execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"15fc699124fc73b2974081fd6e85752cbd9cd440","id":"oeY5VWJwycsw","executionInfo":{"status":"aborted","timestamp":1701508207371,"user_tz":-330,"elapsed":32,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":[],"execution_count":null,"outputs":[]},{"metadata":{"id":"gmMeb4METhm2","_uuid":"2898911f7af52671923cf6bbc8cab6bc7871aa59","executionInfo":{"status":"aborted","timestamp":1701508207371,"user_tz":-330,"elapsed":32,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":["## Latent Dirichlet Allocation (LDA)"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd861ec68f8272be78421f8bd76b98793009e836","id":"n1EmRI5lycsx"},"cell_type":"markdown","source":["LDA is the most popular technique.**The topics then generate words based on their probability distribution. Given a dataset of documents, LDA backtracks and tries to figure out what topics would create those documents in the first place.**\n","\n","**To understand the maths it seems as if knowledge of Dirichlet distribution (distribution of distributions) is required which is quite intricate and left fior now.**\n","\n","To get an inituitive explanation of LDA checkout these blogs: [this](https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/)  ,  [this](https://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/)  ,[this](https://en.wikipedia.org/wiki/Topic_model)  ,  [this kernel on Kaggle](https://www.kaggle.com/arthurtok/spooky-nlp-and-topic-modelling-tutorial)  ,  [this](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/) ."]},{"metadata":{"id":"yPJFHVxxTiwh","trusted":false,"_uuid":"b21a29802cac8f469c65df49ba0ef04f592f3202","executionInfo":{"status":"aborted","timestamp":1701508207371,"user_tz":-330,"elapsed":32,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":["from sklearn.decomposition import LatentDirichletAllocation\n","lda_model=LatentDirichletAllocation(n_components=10,learning_method='online',random_state=42,max_iter=1)\n","# n_components is the number of topics"],"execution_count":null,"outputs":[]},{"metadata":{"id":"aeUPTUUIazvB","trusted":false,"_uuid":"8c0169b02a356e8cf8583218437d23f549c1e3da","executionInfo":{"status":"aborted","timestamp":1701508207371,"user_tz":-330,"elapsed":32,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":["lda_top=lda_model.fit_transform(vect_text)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"h8LzsFixdkBt","trusted":false,"_uuid":"77425315af5c000fa30caefd6dcebb70c2970cfa","executionInfo":{"status":"aborted","timestamp":1701508207371,"user_tz":-330,"elapsed":32,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":["print(lda_top.shape)  # (no_of_doc,no_of_topics)\n","print(lda_top)\n"],"execution_count":null,"outputs":[]},{"metadata":{"id":"YNgP0S0eeVuO","trusted":false,"_uuid":"2fc450b8316e62d050eb702b65eddbb1e898248f","executionInfo":{"status":"aborted","timestamp":1701508207372,"user_tz":-330,"elapsed":33,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":["sum=0\n","for i in lda_top[0]:\n","  sum=sum+i\n","print(sum)"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f6d8a55e8a513a9af61ae480eae5a0e2135f182","id":"hPgSmMxNycsy"},"cell_type":"markdown","source":["#### Note that the values in a particular row adds to 1. This is beacuse each value denotes the % of contribution of the corressponding topic in the document."]},{"metadata":{"id":"Z6WmZLp3ehbY","trusted":false,"_uuid":"d896b8fcf2c398367de4e7b7142e144fb4944f2d","executionInfo":{"status":"aborted","timestamp":1701508207372,"user_tz":-330,"elapsed":33,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":["# composition of doc 0 for eg\n","print(\"Document 0: \")\n","for i,topic in enumerate(lda_top[0]):\n","  print(\"Topic \",i,\": \",topic*100,\"%\")"],"execution_count":null,"outputs":[]},{"metadata":{"id":"NVgtw_L_g8N2","_uuid":"d24927b415ef64338a4b99e81c9cafd5225dfae5","executionInfo":{"status":"aborted","timestamp":1701508207372,"user_tz":-330,"elapsed":33,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":["#### As we can see Topic 7 & 8 are dominantly present in document 0.\n","\n",""],"execution_count":null,"outputs":[]},{"metadata":{"id":"6OFxmyReiZIU","trusted":false,"_uuid":"3ee132a423ca0bfe3107fa3dd2652d6c33d5bc4d","executionInfo":{"status":"aborted","timestamp":1701508207372,"user_tz":-330,"elapsed":33,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":["print(lda_model.components_)\n","print(lda_model.components_.shape)  # (no_of_topics*no_of_words)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"NoEr9qt1jgsM","_uuid":"8bf169b07cefbe2cd8e36bc1e2bd65ca7c398374","executionInfo":{"status":"aborted","timestamp":1701508207372,"user_tz":-330,"elapsed":32,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":["#### Most important words for a topic. (say 10 this time.)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"hKJHM0C-l0an","trusted":false,"_uuid":"5c13c5563657ac5103a33615272320065542fb40","executionInfo":{"status":"aborted","timestamp":1701508207372,"user_tz":-330,"elapsed":32,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":["# most important words for each topic\n","vocab = vect.get_feature_names()\n","\n","for i, comp in enumerate(lda_model.components_):\n","    vocab_comp = zip(vocab, comp)\n","    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]\n","    print(\"Topic \"+str(i)+\": \")\n","    for t in sorted_words:\n","        print(t[0],end=\" \")\n","    print(\"\\n\")"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a094d2d6e95dc236b2bd3d70c69d1b3767cb4cea","id":"BmepzAKnycsz","executionInfo":{"status":"aborted","timestamp":1701508207372,"user_tz":-330,"elapsed":32,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":[],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"854f0eb88cf78923db8137a2f41581eb9bbd5592","id":"Zw2EUhAjycs0"},"cell_type":"markdown","source":["#### To better visualize words in a topic we can see the word cloud. For each topic top 50 words are plotted."]},{"metadata":{"id":"_ac73PUhmZmn","trusted":false,"_uuid":"5221ae4de021107ef757b87d45053460e50582c8","executionInfo":{"status":"aborted","timestamp":1701508207372,"user_tz":-330,"elapsed":32,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":["from wordcloud import WordCloud\n","# Generate a word cloud image for given topic\n","def draw_word_cloud(index):\n","  imp_words_topic=\"\"\n","  comp=lda_model.components_[index]\n","  vocab_comp = zip(vocab, comp)\n","  sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:50]\n","  for word in sorted_words:\n","    imp_words_topic=imp_words_topic+\" \"+word[0]\n","\n","  wordcloud = WordCloud(width=600, height=400).generate(imp_words_topic)\n","  plt.figure( figsize=(5,5))\n","  plt.imshow(wordcloud)\n","  plt.axis(\"off\")\n","  plt.tight_layout()\n","  plt.show()\n",""],"execution_count":null,"outputs":[]},{"metadata":{"id":"-tD4nZdRqnAk","trusted":false,"_uuid":"e5246db8bb1b751a1f3b96bb65d4da9e34dfc2ba","executionInfo":{"status":"aborted","timestamp":1701508207373,"user_tz":-330,"elapsed":33,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":["# topic 0\n","draw_word_cloud(0)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"Jz_zGbSws1ns","trusted":false,"_uuid":"74872c17b12680171090690f62ce77019e03ef52","executionInfo":{"status":"aborted","timestamp":1701508207373,"user_tz":-330,"elapsed":33,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":["# topic 1\n","draw_word_cloud(1)  # ..."],"execution_count":null,"outputs":[]},{"metadata":{"id":"gkhvmGMds6BV","trusted":false,"_uuid":"a71cd3aa1fa1834f11023bbd98a0a4b558f5b8c7","executionInfo":{"status":"aborted","timestamp":1701508207373,"user_tz":-330,"elapsed":33,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":[],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e5a0755a89a138ba157a57502bb6f6d8df8577a","id":"nY3aZRrmycs1"},"cell_type":"markdown","source":["## THE END !!!"]},{"metadata":{"_uuid":"bae1e1f367c3c9b3ebc8b60bc5c3568c131fb1d9","id":"ughaSHZZycs1"},"cell_type":"markdown","source":["## [Please star/upvote in case u liked it. ]"]},{"metadata":{"trusted":false,"_uuid":"220f0c0b4fc78848f66ee48aa11b91c3f3f7838a","id":"okqZCzrdycs1","executionInfo":{"status":"aborted","timestamp":1701508207373,"user_tz":-330,"elapsed":33,"user":{"displayName":"Eisha Akanskha","userId":"01502528335957365622"}}},"cell_type":"code","source":[],"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}